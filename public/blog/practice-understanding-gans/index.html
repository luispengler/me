<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Practice: Understanding GANs With MNIST Dataset | Luis Spengler - Webpage</title><meta name=keywords content><meta name=description content="Introduction In this blog post I will walk you through creating your first GAN for MNIST dataset image generation. We will be running our code on Google Colab, but if you have access to a GPU or any other server feel free to do it there, the code will be pretty much the same.
If you are new to the MNIST dataset, I can sum it up saying it is a dataset containing 70k handwritten numbers ranging from 0 to 9, as seen in the picture below."><meta name=author content="Me"><link rel=canonical href=https://luispengler.github.io/me/blog/practice-understanding-gans/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/me/assets/css/stylesheet.6a98292fb8fa8cf0f3ba4042d4b75515c04267550f3ad49ff6271b5af9562443.css integrity="sha256-apgpL7j6jPDzukBC1LdVFcBCZ1UPOtSf9icbWvlWJEM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/me/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://luispengler.github.io/me/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://luispengler.github.io/me/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://luispengler.github.io/me/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://luispengler.github.io/me/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://luispengler.github.io/me/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Practice: Understanding GANs With MNIST Dataset"><meta property="og:description" content="Introduction In this blog post I will walk you through creating your first GAN for MNIST dataset image generation. We will be running our code on Google Colab, but if you have access to a GPU or any other server feel free to do it there, the code will be pretty much the same.
If you are new to the MNIST dataset, I can sum it up saying it is a dataset containing 70k handwritten numbers ranging from 0 to 9, as seen in the picture below."><meta property="og:type" content="article"><meta property="og:url" content="https://luispengler.github.io/me/blog/practice-understanding-gans/"><meta property="og:image" content="https://luispengler.github.io/me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2023-03-15T23:38:34-04:00"><meta property="article:modified_time" content="2023-03-15T23:38:34-04:00"><meta property="og:site_name" content="Luis Spengler - Webpage"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://luispengler.github.io/me/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Practice: Understanding GANs With MNIST Dataset"><meta name=twitter:description content="Introduction In this blog post I will walk you through creating your first GAN for MNIST dataset image generation. We will be running our code on Google Colab, but if you have access to a GPU or any other server feel free to do it there, the code will be pretty much the same.
If you are new to the MNIST dataset, I can sum it up saying it is a dataset containing 70k handwritten numbers ranging from 0 to 9, as seen in the picture below."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blog","item":"https://luispengler.github.io/me/blog/"},{"@type":"ListItem","position":3,"name":"Practice: Understanding GANs With MNIST Dataset","item":"https://luispengler.github.io/me/blog/practice-understanding-gans/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Practice: Understanding GANs With MNIST Dataset","name":"Practice: Understanding GANs With MNIST Dataset","description":"Introduction In this blog post I will walk you through creating your first GAN for MNIST dataset image generation. We will be running our code on Google Colab, but if you have access to a GPU or any other server feel free to do it there, the code will be pretty much the same.\nIf you are new to the MNIST dataset, I can sum it up saying it is a dataset containing 70k handwritten numbers ranging from 0 to 9, as seen in the picture below.","keywords":[],"articleBody":"Introduction In this blog post I will walk you through creating your first GAN for MNIST dataset image generation. We will be running our code on Google Colab, but if you have access to a GPU or any other server feel free to do it there, the code will be pretty much the same.\nIf you are new to the MNIST dataset, I can sum it up saying it is a dataset containing 70k handwritten numbers ranging from 0 to 9, as seen in the picture below.\nOur GAN model will be used for creating images that ressamble those from the MNIST dataset, which in turn means that by the end of this practice we will have created a GAN that can write down some numbers :)\nExplaining the model Before going through the code it is important we understand better what we will be creating.\nAs you may know GANs are composed by two neural networks: Generator and Discriminator.\nThe Discriminator’s goal is to be able to tell if a given input is deemed real rather than fake, providing a probability of the input being either real or fake.\nThe Generator’s goal is to create images that will fool the Discriminator by the latter saying the image has a high probability of being real.\nIn the training process, we only update the weights and biases of one of the models. If we are training the Discriminator, we will not touch on the settings of our Generator network. The same is valid for the Generator. In more details that’s the training process:\nThe Discriminator takes examples of images from a real dataset (X), and from a fake dataset (X*) that we are generating. Then it computes how much it has mistaken the classifications, which is something we call the Discriminator’s loss, and updates its own weights and biases so that next time it has a smaller loss value (minimizing its own loss). In this part the Generator remains unaltered.\nThe Generator takes input from a random noise source (z), and it produces a fake image as output (X*). Then it computes how much the Discriminator has mistaken the classification of the fake images, and updates its weights and biases to maximize the Discriminator’s loss. This time, the Discriminator remains unaltered.\nAbove you can see the GAN structure we will be creating, which is the summary of what I just described. It is a very general GAN model, but it will be sufficient for generating an understanding of this kind of machine learning framework.\nHands-on! In this session I will be breaking down some parts of the code that are generally confusing, but if you are ready you can have the full code, without unnecessary parts, in the next session.\nThe MNIST dataset images Let’s first get an understanding of the MNIST dataset images.\n1 2 from keras.datasets import mnist (X_train, y_train), (X_test, y_test) = mnist.load_data() Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11490434/11490434 [==============================] - 0s 0us/step We have a very nicely structured numpy array as our training dataset.\n1 type(X_train) numpy.ndarray Having a look into the dimensions. Below 60000 means the amount of images we have. In the case of X_train we have 60k images. 28, 28 correspond to the height and width (AKA dimensions) of each one of these 60k images.\n1 X_train.shape (60000, 28, 28) We can even confirm this by getting the dimensions of one image.\n1 X_train[0].shape (28, 28) Then plotting it so we see it.\n1 2 import matplotlib.pyplot as plt plt.imshow(X_train[0], cmap='gray' Now that you understood we are working with 28x28 images, it is time to define the output dimensions to our generator/input dimensions to our discriminator. That is saying they will be working with 28x28 images.\nNotice we are also defining channels below. It is equal to one because we want to work with only one channel of color. If we wanted RGB (Red, Green, Blue), we would define channels equal to 3.\n1 2 3 4 5 6 7 ## Model input dimensions img_rows = 28 img_cols = 28 channels = 1 # Input image dimensions img_shape = (img_rows, img_cols, channels) Here we will also define the size of the random noise source we will be using. It can also be called noise vector as you see in the comment below.\n1 2 # Size of the noise vector, used as input to the Generator z_dim = 100 Generator Short explanation Let’s break down the code for our Generator (you can find it some paragraphs below). In the Fully connected layer part, it takes in the noise vector of size 100 we defined earlier, and connects it to 128 units in our first neural network layer. Then it gets connected to our first and only hidden layer, which is using a Leaky ReLU activation function.\nAn activation function is a function that will put our values in a defined range. The Leaky ReLU doesn’t limit positive numbers, meaning if we put in 16 as a number it wouldn’t do anything to it and would still print out 16. However, for negative numbers it would make them bigger (that is, make them approach zero), in an order of 100 times. The following image may better clarify what Leaky ReLU does, but if you still didn’t understand it, just think of it as a special layer in our neural network that prevents gradients from dying out during training, improving the quality of our Generator.\nThe almost-last layer (Output layer with tanh activation) of our neural network is the one that gives us images, although in the format of 28x28 (which equals 784) by 1, and uses the tanh activation function. This activation function formats the pixel values of the image to be in -1 to 1 range. This gives our generator the ability to produce crisper images.\nLet’s have a better understanding of what I meant by this pixel range (-1 to 1). Every image is made up by numbers, even our beautiful MNIST handwritten digits! To see those numbers, let’s print out the image instead of showing it…\nPreviously we displayed the number five, which is the zeroth image of the MNIST dataset, using matplotlib:\n1 2 import matplotlib.pyplot as plt plt.imshow(X_train[0], cmap='gray') Seeing the numbers that make up this image is easy:\n1 print(X_train[0]) [[ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 3 18 18 18 126 136 175 26 166 255 247 127 0 0 0 0] [ 0 0 0 0 0 0 0 0 30 36 94 154 170 253 253 253 253 253 225 172 253 242 195 64 0 0 0 0] [ 0 0 0 0 0 0 0 49 238 253 253 253 253 253 253 253 253 251 93 82 82 56 39 0 0 0 0 0] [ 0 0 0 0 0 0 0 18 219 253 253 253 253 253 198 182 247 241 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 80 156 107 253 253 205 11 0 43 154 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 14 1 154 253 90 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 139 253 190 2 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 11 190 253 70 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 35 241 225 160 108 1 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 81 240 253 253 119 25 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 45 186 253 253 150 27 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 16 93 252 253 187 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 249 253 249 64 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 46 130 183 253 253 207 2 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 39 148 229 253 253 253 250 182 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 24 114 221 253 253 253 253 201 78 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 23 66 213 253 253 253 253 198 81 2 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 18 171 219 253 253 253 253 195 80 9 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 55 172 226 253 253 253 253 244 133 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 136 253 253 253 212 135 132 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] We can almost see the number five being formed by the combination of the numbers… These numbers are in grayscale (0 to 255 range), and when used cmap='gray' in matplotlib, it will show a black and white image like the one we got before. Now you understand that changing the range of the pixel values, changes the range of the numbers that make up an image (tanh would transform 0 to 255, to -1 to 1). We will see more of this in the training subsection.\nNow the real-last layer (Reshape) of our neural network simply reshapes the images which are in 784 by 1 to our normal size images: 28 by 28. That is, before getting into this last layer, we had some sort of array in the dimensions 784x1. Watch out for the difference:\n1 X_train[0].shape (28, 28) 1 2 3 import numpy as np c = np.reshape(X_train[0], (28*28, 1)) c.shape (784, 1) The reshape layer takes the (784, 1) array and makes it a (28, 28) array that you just saw above!\nCode Let us finally get into the code for creating this Generator.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 from keras.models import Sequential from keras.layers import Dense, Flatten, Reshape from keras.layers import LeakyReLU def build_generator(img_shape, z_dim): model = Sequential() # Fully connected layer model.add(Dense(128, input_dim=z_dim)) # Leaky ReLU activation model.add(LeakyReLU(alpha=0.01)) # Output layer with tanh activation model.add(Dense(28 * 28 * 1, activation='tanh')) # Reshape the Generator output to image dimensions model.add(Reshape(img_shape)) return model Discriminator Short explanation You may notice the Discriminator and the Generator are very similar networks, however in most GANs implementations they often very greatly in both size and complexity.\nThe first layer (Flatten) takes in the images in the format of 28x28 and reshapes them to 784x1. Now you might ask me why we went through the small hassle of adding another layer in our GAN model for reshaping since our work would be “undone” when it got to the input of the discriminator. The reason we reshaped the output of the Generator network was for its images to blend in with the real images which are already in the 28x28 format, regardless of the Discriminator network taking in 784x1 images as input. However, you could in theory simply reshape the MNIST dataset images to 784x1, and not need to reshape the generated images to 28x28 (just leave it at 784x1 as well), then the real images and the fake ones would still blend in (just not in the 28x28 format).\nThen, the reshaped images will go into a Fully connected layer of 128 units. Then it gets connected to our first and only hidden layer that uses a Leaky ReLU activation function. This part is identical to the Generator network.\nThe last layer will output probabilities, therefore we are using the Sigmoid activation function, which maps all the outputs in the range of 0 to 1.\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def build_discriminator(img_shape): model = Sequential() # Flatten the input image model.add(Flatten(input_shape=img_shape)) # Fully connected layer model.add(Dense(128)) # Leaky ReLU activation model.add(LeakyReLU(alpha=0.01)) # Output layer with sigmoid activation model.add(Dense(1, activation='sigmoid')) return model Building the Model Short explanation In build_gan() we add Generator and Discriminator together in our model.\nThe Discriminator is compiled alone, taking as input img_shape (which we defined earlier) and the loss being computed using binary_crossentropy'. Binary cross-entropy is a measure of the difference between computed probabilities and actual probabilities for predictions with only two possible classes. In our case the possible classes are real or fake.\nTherefore, binary cross-entropy is going to tell us how off the prediction of the Discriminator is. Remember the Discriminator’s goal is to not be off in the predictions, while the Generator wants the Discriminator to be very off when it comes to fake images predictions. This also means the Discriminator wants to minimize its loss for real and fake images, and the Generator wants to maximize the Discriminator loss for fake images.\nThe optimizer argument is required for compiling the model, although I don’t know how the Adam() optimizer works, it has become the default in many GAN implementations due to its often superior performance. However, it suffices to say the optimizer is the one who is responsible for updating the weights and learning rates of the neural network.\nThe 'accuracy' metrics is the way the Discriminator, and later us, will know how well it is doing.\nThe Generator is built taking as input img_shape and the z_dim arguments we defined earlier. Then, we only compile the Generator together with the Discriminator in the gan argument. For doing this, we first neet to set discriminator.trainable to false.\nTo clarify what I just stated, notice how below we built and compiled the Discriminator alone. This means that when we use the Discriminator and train it, it will take care of its own weights without interfering with the Generator network. Since it is alone, it can’t interfere with anyone else. In turn, the generator is built alone, but it is compiled only when it is together with the Discriminator. It is only possible to train the Generator without tweaking the Discriminator weights by mistake if we set discriminator.trainable to false.\nLastly, we save a file called generator_model.h5 containing the Generator network model. After trained, we can even use this file in other projects with the same abilities our generative model will have.\nCode 1 2 3 4 5 6 7 8 9 def build_gan(generator, discriminator): model = Sequential() # Combined Generator -\u003e Discriminator model model.add(generator) model.add(discriminator) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from keras.optimizers import Adam # Build and compile the Discriminator discriminator = build_discriminator(img_shape) discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy']) # Build the Generator generator = build_generator(img_shape, z_dim) # Keep Discriminator’s parameters constant for Generator training discriminator.trainable = False # Build and compile GAN model with fixed Discriminator to train the Generator gan = build_gan(generator, discriminator) gan.compile(loss='binary_crossentropy', optimizer=Adam()) generator.save('generator_model.h5') WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model. Training Short explanation Next, we will build a training loop for our GAN.\nThe following command only gives us the X_train dataset from MNIST dataset. You can see that all the 60k images in the format of 28x28 are there.\n1 2 (X_train, _), (_, _) = mnist.load_data() X_train.shape (60000, 28, 28) However, they are still in the 0 to 255 range we saw earlier.\n1 print(X_train[0]) [[ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 3 18 18 18 126 136 175 26 166 255 247 127 0 0 0 0] [ 0 0 0 0 0 0 0 0 30 36 94 154 170 253 253 253 253 253 225 172 253 242 195 64 0 0 0 0] [ 0 0 0 0 0 0 0 49 238 253 253 253 253 253 253 253 253 251 93 82 82 56 39 0 0 0 0 0] [ 0 0 0 0 0 0 0 18 219 253 253 253 253 253 198 182 247 241 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 80 156 107 253 253 205 11 0 43 154 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 14 1 154 253 90 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 139 253 190 2 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 11 190 253 70 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 35 241 225 160 108 1 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 81 240 253 253 119 25 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 45 186 253 253 150 27 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 16 93 252 253 187 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 249 253 249 64 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 46 130 183 253 253 207 2 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 39 148 229 253 253 253 250 182 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 24 114 221 253 253 253 253 201 78 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 23 66 213 253 253 253 253 198 81 2 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 18 171 219 253 253 253 253 195 80 9 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 55 172 226 253 253 253 253 244 133 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 136 253 253 253 212 135 132 16 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] The issue is that the real images are very distinguishable from the fake ones, since we used tanh activation function in the Generator, it is working with images that go from -1 to 1. That means that the fake images are in this -1 to 1 range, while the real ones are in 0 to 255. Very easy to spot their differences like this. Let us fix this by setting the range of values of the real images to also be in -1 to 1.\n1 2 X_train = X_train / 127.5 - 1.0 X_train = np.expand_dims(X_train, axis=3) Now we print another 5 from the real dataset to check if they are correct…\n1 print(X_train[0]) [[[-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ]] [[-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ]] [[-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ]] [[-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ]] [[-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ]] [[-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-0.97647059] [-0.85882353] [-0.85882353] [-0.85882353] [-0.01176471] [ 0.06666667] [ 0.37254902] [-0.79607843] [ 0.30196078] [ 1. ] [ 0.9372549 ] [-0.00392157] [-1. ] [-1. ] [-1. ] [-1. ]] [[-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-0.76470588] [-0.71764706] [-0.2627451 ] [ 0.20784314] [ 0.33333333] [ 0.98431373] [ 0.98431373] [ 0.98431373] [ 0.98431373] [ 0.98431373] [ 0.76470588] [ 0.34901961] [ 0.98431373] [ 0.89803922] [ 0.52941176] [-0.49803922] [-1. ] [-1. ] [-1. ] [-1. ]] [[-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-0.61568627] [ 0.86666667] [ 0.98431373] [ 0.98431373] [ 0.98431373] [ 0.98431373] [ 0.98431373] [ 0.98431373] [ 0.98431373] [ 0.98431373] [ 0.96862745] [-0.27058824] [-0.35686275] [-0.35686275] [-0.56078431] [-0.69411765] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ]] [[...19 OTHERS...]] [[-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ] [-1. ]]] Woosh, that was big. I even had to truncate 19 of these so it wouldn’t bother you too much. However, notice in the ones that remained how they were never less than -1 or greater than 1. It doesn’t look like a five because we lost the spaces when printing it out, but what matters is the numbers and that this image is still 28x28. I removed 19 units from the rows and columns, but if you count the ones that remained (that is 9 units), you realize that they total 28 (19+9). In fact, if I try to render this -1 to 1 image again, you will see it is still a five!\n1 2 import matplotlib.pyplot as plt plt.imshow(X_train[0], cmap='gray') Next with real = np.ones((batch_size, 1)) and fake = np.zeros((batch_size, 1)) we are creating a 1-dimensional numpy array that will be our labels for real and fake images. As the code suggest, we are encoding real as 1 and fake as 0.\nTraining for loop Our training for loop is very important. It defines the steps that will be followed during training.\nFirst, we are training the discriminator. We get a random batch of real images. The code line idx = np.random.randint(0, X_train.shape[0], batch_size) will give an array of random numbers any time it is read. Mine for once was the below, which you can see by running idx.\n1 idx array([25920, 26064, 33671, 32605, 6425, 59315, 30511, 55435, 26916, 19323, 18101, 23335, 9861, 31422, 28701, 48697, 17212, 47408, 51358, 6759, 58100, 26888, 15078, 18564, 21526, 30725, 24117, 32581, 53086, 6893, 15341, 6371, 41043, 21601, 42828, 28187, 1138, 52940, 18744, 54318, 46527, 42070, 34308, 32423, 45819, 26281, 5715, 25884, 12345, 8417, 46373, 40419, 44005, 25802, 52323, 36148, 34411, 59891, 48229, 4545, 54343, 51594, 39092, 53981, 7812, 58982, 32166, 14466, 11552, 54139, 8675, 11207, 52915, 13795, 38792, 7056, 33569, 36693, 16911, 57138, 15059, 46433, 41947, 21047, 52353, 58981, 57461, 29683, 35477, 42228, 47068, 56887, 43435, 15456, 24735, 53753, 29363, 31095, 54618, 40214, 54801, 40577, 56503, 31241, 8541, 10678, 58001, 43211, 13343, 22059, 28042, 15490, 28817, 50619, 42503, 22699, 11541, 21757, 22338, 15620, 53041, 35000, 11242, 9518, 50624, 55582, 47070, 1390]) So that we can get a random batch of real images, we just assign each one of the numbers in the array you saw above to an index of an image present in the MNIST dataset. This is done by running imgs = X_train[idx]\nSimilarly, by running z = np.random.normal(0, 1, (batch_size, 100)) we get random values for the input of the generator. This is our random noise sourze (z) commented earlier. It generates 100 random numbers in the range from 0 to 1. All of this is necessary so we get randomness in the number generation. No two generated numbers will be identical because everytime z = np.random.normal(0, 1, (batch_size, 100)) is running, we get different values in the array. You can verify what these values are by running the below.\n1 z array([[-1.91992421e-01, -1.08401678e+00, -1.19431854e-03, ..., 3.47946637e-01, 7.05697133e-02, 1.31822535e+00], [ 8.09580777e-01, -8.33102821e-01, 8.19856365e-01, ..., -2.71591923e-01, -4.70752500e-01, -6.09906282e-01], [ 4.79368301e-01, -6.22733779e-01, 6.72358083e-01, ..., 3.19932660e-01, -1.12344273e+00, -7.53240611e-01], ..., [ 2.20391394e-01, -1.89958687e-01, 2.17267157e-01, ..., 8.10365364e-01, -6.03790376e-01, 1.17673864e+00], [-1.07111602e+00, -1.99597750e-01, -3.94322883e-01, ..., 7.82795777e-01, 1.43982877e+00, -1.33052956e+00], [ 2.26731520e-01, -3.90780048e-01, -1.55340947e-01, ..., -5.19932543e-02, 5.72968732e-01, -2.68468205e+00]]) We then feed those random numbers in the array z to our generator, which will from them generate new images and save them in an array called gen_imgs. The code for that is gen_imgs = generator.predict(z) and after ran you can verify some properties\n1 gen_imgs = generator.predict(z) 4/4 [==============================] - 0s 3ms/step 1 gen_imgs.shape (128, 28, 28, 1) Above we can see the generated images are in the format of 28x28, have 1 channel and there are 128 of them.\nNext we get the two batches we randomly generated (that is the random sampling we did with MNSIT dataset and the ones created by the generator through inputing random numbers), and ask the discriminator to make predictions on their classifications, calculating how much it has mistaken and updating its weights and biases to correct that.\n1 2 3 d_loss_real = discriminator.train_on_batch(imgs, real) d_loss_fake = discriminator.train_on_batch(gen_imgs, fake) d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake) For the generator training part, not much changes. Except now we don’t care about random sampling through the MNIST dataset, neither updating the discriminator’s weights and biases, afterall this is the generator training!\nWe get another random array of numbers (z) by running z = np.random.normal(0, 1, (batch_size, 100)) and ask the generator to give us images by running gen_imgs = generator.predict(z). All of this we saw in the discriminator training explanation, so I won’t go into details. The difference now is that we are using g_loss = gan.train_on_batch(z, real) to feed images into the compiled gan model, that has both the discriminator and generator. However, it is with the generator we are talking with because we want it to give us predictions on what could those images be (real or fake). Notice the word real at the end of that line of code. We are calling our images real so that the discriminator can be fooled into thinking these are real images.\nThe last step in the Training code is not necessary for training the GANs, but it will be useful for our understanding of them. We will save their progress for later plotting in a graph. We will also take “snapshots” of what the generator is producing so we can see how well it can handwrite… The frequency with which the snapshots are generated is set by the variable sample_interval which we define in the code from the next session.\nCode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 losses = [] accuracies = [] iteration_checkpoints = [] def train(iterations, batch_size, sample_interval): # Load the MNIST dataset (X_train, _), (_, _) = mnist.load_data() # Rescale [0, 255] grayscale pixel values to [-1, 1] X_train = X_train / 127.5 - 1.0 X_train = np.expand_dims(X_train, axis=3) # Labels for real images: all ones real = np.ones((batch_size, 1)) # Labels for fake images: all zeros fake = np.zeros((batch_size, 1)) for iteration in range(iterations): # ------------------------- # Train the Discriminator # ------------------------- # Get a random batch of real images idx = np.random.randint(0, X_train.shape[0], batch_size) imgs = X_train[idx] # Generate a batch of fake images z = np.random.normal(0, 1, (batch_size, 100)) gen_imgs = generator.predict(z) # Train Discriminator d_loss_real = discriminator.train_on_batch(imgs, real) d_loss_fake = discriminator.train_on_batch(gen_imgs, fake) d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake) # --------------------- # Train the Generator # --------------------- # Generate a batch of fake images z = np.random.normal(0, 1, (batch_size, 100)) gen_imgs = generator.predict(z) # Train Generator g_loss = gan.train_on_batch(z, real) if (iteration + 1) % sample_interval == 0: # Save losses and accuracies so they can be plotted after training losses.append((d_loss, g_loss)) accuracies.append(100.0 * accuracy) iteration_checkpoints.append(iteration + 1) # Output training progress print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration + 1, d_loss, 100.0 * accuracy, g_loss)) # Output a sample of generated image sample_images(generator) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def sample_images(generator, image_grid_rows=4, image_grid_columns=4): # Sample random noise z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim)) # Generate images from random noise gen_imgs = generator.predict(z) # Rescale image pixel values to [0, 1] gen_imgs = 0.5 * gen_imgs + 0.5 # Set image grid fig, axs = plt.subplots(image_grid_rows, image_grid_columns, figsize=(4, 4), sharey=True, sharex=True) cnt = 0 for i in range(image_grid_rows): for j in range(image_grid_columns): # Output a grid of images axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray') axs[i, j].axis('off') cnt += 1 Extra explanation I am adding this section to clarify some things and hopefully answer my mentor’s inquiries.\nHe noticed a weirdness in training with the MNIST dataset. There are 10 numbers, ranging from 0 to 9, which means they all look different! And each step of training we are not defining the numbers we are using that time, so we are not getting our generator to be better at generating 1s or 6s. What we are in fact doing is modeling random noise (from our z) with the statistical properties of real data. These properties are the distribution of the numbers that make up the real images, and things like pixel values, and the spatial correlations between neighboring pixels.\nThat means we are not training our GAN to create any number better specifically, but for all of them in some way to end up looking more real. The specificity in our GAN approach would have made the generator network create a one number better than all the others, our eyes passing images from the generated number as real but reproving all the other number generation attempts (why does this 0 look like a 5???)\nIf we wanted better looking 0s, 1s, 2s, 3s… so that they don’t overall look real, but each one of them looked super real we would have to go with another GAN approach known as conditional GAN (cGAN).\nAnother thing he noticed, and hopefully you notice it too is that the snapshot images (which you can see in the next session) are also random. Look at the last two 4x4 grids. If you go in the same position in the two images you will notice the GAN attempted to create something different in there. It is not the same number.\nThis can be explained because of our noise source z that adds randomness in the number generation. Even though I like this answer, if we look into the tensorflow tutorial on Deep Convolutional Generative Adversarial Network, you can see they created a GIF of those images we are plotting. How could they do that if the numbers are in different positions? Maybe the randomness from z is not a good answer afterall.\nActually training + Inspecting Output Short explanation We covered pretty much every part of the code. Now we are just defining the hyperparameters. While machine learning models are usually very sensible to them, our GAN model is simple and therefore is less sensible to have bad hyperparameters. Of course, your images will become pretty bad if you set them badly, but it is not the end of the world. The number of iterations defines how many times we will go through the training loop we just saw a code block ago. It took me one hour to run it on colab, it might take you a different time running somewhere else. An ideal iteration number for this GAN would be 100,000. However, I don’t want to wait 5h just to get images for a practice tutorial… Maybe you don’t want to wait that much time to learn the content either.\nBatch size will tell our training loop how much images to get for the Discriminator to predict at. And lastly sample_interval defines after how many iterations to print us a 4x4 grid with the progress of the Generator network. That is our “snapshot” commented briefly in the last session.\nAlso, if you get a warning running this part of the code as Discrepancy between trainable weights and collected trainable, it is just Keras complaining we held the Discriminator’s parameters constant while training the Generator.\nThe last two blocks of code are simply the ones that will use matplotlib to output us the 4x4 grid every sample_interval\nCode 1 2 3 4 5 6 7 # Set hyperparameters iterations = 20000 # It takes 1h to run because of this high amount of interations batch_size = 128 sample_interval = 1000 # Train the GAN for the specified number of iterations train(iterations, batch_size, sample_interval Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11490434/11490434 [==============================] - 0s 0us/step 4/4 [==============================] - 0s 4ms/step 4/4 [==============================] - 0s 4ms/step 4/4 [==============================] - 0s 6ms/step 4/4 [==============================] - 0s 4ms/step 4/4 [==============================] - 0s 4ms/step 4/4 [==============================] - 0s 4ms/step 4/4 [==============================] - 0s 4ms/step 4/4 [==============================] - 0s 5ms/step 4/4 [==============================] - 0s 3ms/step 4/4 [==============================] - 0s 3ms/step ... ALSO TRUNCATED HERE SO IT DOESN'T TAKE MUCH SPACE... 1 2 3 4 5 6 7 8 9 10 11 12 13 losses = np.array(losses) # Plot training losses for Discriminator and Generator plt.figure(figsize=(15, 5)) plt.plot(iteration_checkpoints, losses.T[0], label=\"Discriminator loss\") plt.plot(iteration_checkpoints, losses.T[1], label=\"Generator loss\") plt.xticks(iteration_checkpoints, rotation=90) plt.title(\"Training Loss\") plt.xlabel(\"Iteration\") plt.ylabel(\"Loss\") plt.legend() 1 2 3 4 5 6 7 8 9 10 11 12 13 accuracies = np.array(accuracies) # Plot Discriminator accuracy plt.figure(figsize=(15, 5)) plt.plot(iteration_checkpoints, accuracies, label=\"Discriminator accuracy\") plt.xticks(iteration_checkpoints, rotation=90) plt.yticks(range(0, 100, 5)) plt.title(\"Discriminator Accuracy\") plt.xlabel(\"Iteration\") plt.ylabel(\"Accuracy (%)\") plt.legend() Full Code In case you already understand the whole code structure, feel free to just run the code provided below.\nThe code is also available here as a jupyter notebook.\nImports 1 2 3 4 5 6 7 8 9 10 11 # Import statements %matplotlib inline import matplotlib.pyplot as plt import numpy as np from keras.datasets import mnist from keras.layers import Dense, Flatten, Reshape from keras.layers import LeakyReLU from keras.models import Sequential from keras.optimizers import Adam 1 2 3 4 5 6 7 8 9 10 ### Model input dimensions img_rows = 28 img_cols = 28 channels = 1 # Input image dimensions img_shape = (img_rows, img_cols, channels) # Size of the noise vector, used as input to the Generator z_dim = 100 Generator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def build_generator(img_shape, z_dim): model = Sequential() # Fully connected layer model.add(Dense(128, input_dim=z_dim)) # Leaky ReLU activation model.add(LeakyReLU(alpha=0.01)) # Output layer with tanh activation model.add(Dense(28 * 28 * 1, activation='tanh')) # Reshape the Generator output to image dimensions model.add(Reshape(img_shape)) return model Discriminator 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def build_discriminator(img_shape): model = Sequential() # Flatten the input image model.add(Flatten(input_shape=img_shape)) # Fully connected layer model.add(Dense(128)) # Leaky ReLU activation model.add(LeakyReLU(alpha=0.01)) # Output layer with sigmoid activation model.add(Dense(1, activation='sigmoid')) return model Building the Model 1 2 3 4 5 6 7 8 9 def build_gan(generator, discriminator): model = Sequential() # Combined Generator -\u003e Discriminator model model.add(generator) model.add(discriminator) return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Build and compile the Discriminator discriminator = build_discriminator(img_shape) discriminator.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy']) # Build the Generator generator = build_generator(img_shape, z_dim) # Keep Discriminator’s parameters constant for Generator training discriminator.trainable = False # Build and compile GAN model with fixed Discriminator to train the Generator gan = build_gan(generator, discriminator) gan.compile(loss='binary_crossentropy', optimizer=Adam()) generator.save('generator_model.h5') Training 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 losses = [] accuracies = [] iteration_checkpoints = [] def train(iterations, batch_size, sample_interval): # Load the MNIST dataset (X_train, _), (_, _) = mnist.load_data() # Rescale [0, 255] grayscale pixel values to [-1, 1] X_train = X_train / 127.5 - 1.0 X_train = np.expand_dims(X_train, axis=3) # Labels for real images: all ones real = np.ones((batch_size, 1)) # Labels for fake images: all zeros fake = np.zeros((batch_size, 1)) for iteration in range(iterations): # ------------------------- # Train the Discriminator # ------------------------- # Get a random batch of real images idx = np.random.randint(0, X_train.shape[0], batch_size) imgs = X_train[idx] # Generate a batch of fake images z = np.random.normal(0, 1, (batch_size, 100)) gen_imgs = generator.predict(z) # Train Discriminator d_loss_real = discriminator.train_on_batch(imgs, real) d_loss_fake = discriminator.train_on_batch(gen_imgs, fake) d_loss, accuracy = 0.5 * np.add(d_loss_real, d_loss_fake) # --------------------- # Train the Generator # --------------------- # Generate a batch of fake images z = np.random.normal(0, 1, (batch_size, 100)) gen_imgs = generator.predict(z) # Train Generator g_loss = gan.train_on_batch(z, real) if (iteration + 1) % sample_interval == 0: # Save losses and accuracies so they can be plotted after training losses.append((d_loss, g_loss)) accuracies.append(100.0 * accuracy) iteration_checkpoints.append(iteration + 1) # Output training progress print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration + 1, d_loss, 100.0 * accuracy, g_loss)) # Output a sample of generated image sample_images(generator) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def sample_images(generator, image_grid_rows=4, image_grid_columns=4): # Sample random noise z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim)) # Generate images from random noise gen_imgs = generator.predict(z) # Rescale image pixel values to [0, 1] gen_imgs = 0.5 * gen_imgs + 0.5 # Set image grid fig, axs = plt.subplots(image_grid_rows, image_grid_columns, figsize=(4, 4), sharey=True, sharex=True) cnt = 0 for i in range(image_grid_rows): for j in range(image_grid_columns): # Output a grid of images axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray') axs[i, j].axis('off') cnt += 1 Actually training + Inspecting Output Note that the 'Discrepancy between trainable weights and collected trainable' warning from Keras is expected. It is by design: The Generator’s trainable parameters are intentionally held constant during Discriminator training, and vice versa.\n1 2 3 4 5 6 7 # Set hyperparameters iterations = 20000 # It takes 1h to run because of this high amount of interations batch_size = 128 sample_interval = 1000 # Train the GAN for the specified number of iterations train(iterations, batch_size, sample_interval) 1 2 3 4 5 6 7 8 9 10 11 12 13 losses = np.array(losses) # Plot training losses for Discriminator and Generator plt.figure(figsize=(15, 5)) plt.plot(iteration_checkpoints, losses.T[0], label=\"Discriminator loss\") plt.plot(iteration_checkpoints, losses.T[1], label=\"Generator loss\") plt.xticks(iteration_checkpoints, rotation=90) plt.title(\"Training Loss\") plt.xlabel(\"Iteration\") plt.ylabel(\"Loss\") plt.legend() 1 2 3 4 5 6 7 8 9 10 11 12 13 accuracies = np.array(accuracies) # Plot Discriminator accuracy plt.figure(figsize=(15, 5)) plt.plot(iteration_checkpoints, accuracies, label=\"Discriminator accuracy\") plt.xticks(iteration_checkpoints, rotation=90) plt.yticks(range(0, 100, 5)) plt.title(\"Discriminator Accuracy\") plt.xlabel(\"Iteration\") plt.ylabel(\"Accuracy (%)\") plt.legend() ","wordCount":"7415","inLanguage":"en","datePublished":"2023-03-15T23:38:34-04:00","dateModified":"2023-03-15T23:38:34-04:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://luispengler.github.io/me/blog/practice-understanding-gans/"},"publisher":{"@type":"Organization","name":"Luis Spengler - Webpage","logo":{"@type":"ImageObject","url":"https://luispengler.github.io/me/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://luispengler.github.io/me accesskey=h title="Home (Alt + H)"><img src=https://luispengler.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://luispengler.github.io/me/books/ title=Books><span>Books</span></a></li><li><a href=https://luispengler.github.io/me/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://luispengler.github.io/me/notes/ title=Notes><span>Notes</span></a></li><li><a href=https://luispengler.github.io/me/categories/ title=categories><span>categories</span></a></li><li><a href=https://luispengler.github.io/me/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://luispengler.github.io/me>Home</a>&nbsp;»&nbsp;<a href=https://luispengler.github.io/me/blog/>Blog</a></div><h1 class=post-title>Practice: Understanding GANs With MNIST Dataset</h1><div class=post-meta><span title='2023-03-15 23:38:34 -0400 -04'>March 15, 2023</span>&nbsp;·&nbsp;35 min&nbsp;·&nbsp;7415 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/luispengler/me/content/blog/practice-understanding-gans.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#explaining-the-model>Explaining the model</a></li><li><a href=#hands-on>Hands-on!</a><ul><li><a href=#the-mnist-dataset-images>The MNIST dataset images</a></li><li><a href=#generator>Generator</a></li><li><a href=#discriminator>Discriminator</a></li><li><a href=#building-the-model>Building the Model</a></li><li><a href=#training>Training</a></li><li><a href=#actually-training--inspecting-output>Actually training + Inspecting Output</a></li></ul></li><li><a href=#full-code>Full Code</a><ul><li><a href=#imports>Imports</a></li><li><a href=#generator-1>Generator</a></li><li><a href=#discriminator-1>Discriminator</a></li><li><a href=#building-the-model-1>Building the Model</a></li><li><a href=#training-1>Training</a></li><li><a href=#actually-training--inspecting-output-1>Actually training + Inspecting Output</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>In this blog post I will walk you through creating your first GAN for MNIST dataset image generation. We will be running our code on Google Colab, but if you have access to a GPU or any other server feel free to do it there, the code will be pretty much the same.</p><p>If you are new to the MNIST dataset, I can sum it up saying it is a dataset containing 70k handwritten numbers ranging from 0 to 9, as seen in the picture below.</p><p><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/example-mnist.jpg?raw=true" alt="MNIST Dataset"></p><p>Our GAN model will be used for creating images that ressamble those from the MNIST dataset, which in turn means that by the end of this practice we will have created a GAN that can write down some numbers :)</p><h2 id=explaining-the-model>Explaining the model<a hidden class=anchor aria-hidden=true href=#explaining-the-model>#</a></h2><p>Before going through the code it is important we understand better what we will be creating.</p><p>As you may know GANs are composed by two neural networks: Generator and Discriminator.</p><p>The Discriminator&rsquo;s goal is to be able to tell if a given input is deemed real rather than fake, providing a probability of the input being either real or fake.</p><p>The Generator&rsquo;s goal is to create images that will fool the Discriminator by the latter saying the image has a high probability of being real.</p><p>In the training process, we only update the weights and biases of one of the models. If we are training the Discriminator, we will not touch on the settings of our Generator network. The same is valid for the Generator. In more details that&rsquo;s the training process:</p><p>The Discriminator takes examples of images from a real dataset (X), and from a fake dataset (X*) that we are generating. Then it computes how much it has mistaken the classifications, which is something we call the Discriminator&rsquo;s loss, and updates its own weights and biases so that next time it has a smaller loss value (minimizing its own loss). In this part the Generator remains unaltered.</p><p>The Generator takes input from a random noise source (z), and it produces a fake image as output (X*). Then it computes how much the Discriminator has mistaken the classification of the fake images, and updates its weights and biases to maximize the Discriminator&rsquo;s loss. This time, the Discriminator remains unaltered.</p><p><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan-layout.png?raw=true" alt="MNIST Dataset">
Above you can see the GAN structure we will be creating, which is the summary of what I just described. It is a very general GAN model, but it will be sufficient for generating an understanding of this kind of machine learning framework.</p><h2 id=hands-on>Hands-on!<a hidden class=anchor aria-hidden=true href=#hands-on>#</a></h2><p>In this session I will be breaking down some parts of the code that are generally confusing, but if you are ready you can have the full code, without unnecessary parts, in the <a href=#full-code>next session</a>.</p><h3 id=the-mnist-dataset-images>The MNIST dataset images<a hidden class=anchor aria-hidden=true href=#the-mnist-dataset-images>#</a></h3><p>Let&rsquo;s first get an understanding of the MNIST dataset images.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.datasets</span> <span class=kn>import</span> <span class=n>mnist</span>
</span></span><span class=line><span class=cl><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>y_train</span><span class=p>),</span> <span class=p>(</span><span class=n>X_test</span><span class=p>,</span> <span class=n>y_test</span><span class=p>)</span> <span class=o>=</span> <span class=n>mnist</span><span class=o>.</span><span class=n>load_data</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
</span></span><span class=line><span class=cl>11490434/11490434 [==============================] - 0s 0us/step</span></span></code></pre></div><p>We have a very nicely structured numpy array as our training dataset.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>type</span><span class=p>(</span><span class=n>X_train</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>numpy.ndarray</span></span></code></pre></div><p>Having a look into the dimensions. Below 60000 means the amount of images we have. In the case of X_train we have 60k images. 28, 28 correspond to the height and width (AKA dimensions) of each one of these 60k images.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X_train</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>(60000, 28, 28)</span></span></code></pre></div><p>We can even confirm this by getting the dimensions of one image.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X_train</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>(28, 28)</span></span></code></pre></div><p>Then plotting it so we see it.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>X_train</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span>
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/x_train[0].png?raw=true" alt="MNIST Dataset"></p><p>Now that you understood we are working with 28x28 images, it is time to define the output dimensions to our generator/input dimensions to our discriminator. That is saying they will be working with 28x28 images.</p><p>Notice we are also defining channels below. It is equal to one because we want to work with only one channel of color. If we wanted RGB (Red, Green, Blue), we would define channels equal to 3.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>## Model input dimensions</span>
</span></span><span class=line><span class=cl><span class=n>img_rows</span> <span class=o>=</span> <span class=mi>28</span>
</span></span><span class=line><span class=cl><span class=n>img_cols</span> <span class=o>=</span> <span class=mi>28</span>
</span></span><span class=line><span class=cl><span class=n>channels</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Input image dimensions</span>
</span></span><span class=line><span class=cl><span class=n>img_shape</span> <span class=o>=</span> <span class=p>(</span><span class=n>img_rows</span><span class=p>,</span> <span class=n>img_cols</span><span class=p>,</span> <span class=n>channels</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Here we will also define the size of the random noise source we will be using. It can also be called noise vector as you see in the comment below.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Size of the noise vector, used as input to the Generator</span>
</span></span><span class=line><span class=cl><span class=n>z_dim</span> <span class=o>=</span> <span class=mi>100</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=generator>Generator<a hidden class=anchor aria-hidden=true href=#generator>#</a></h3><h4 id=short-explanation>Short explanation<a hidden class=anchor aria-hidden=true href=#short-explanation>#</a></h4><p>Let&rsquo;s break down the code for our Generator (<a href=#code>you can find it some paragraphs below</a>). In the <code>Fully connected layer</code> part, it takes in the noise vector of size 100 we defined earlier, and connects it to 128 units in our first neural network layer. Then it gets connected to our first and only hidden layer, which is using a <code>Leaky ReLU activation</code> function.</p><p>An activation function is a function that will put our values in a defined range. The Leaky ReLU doesn&rsquo;t limit positive numbers, meaning if we put in 16 as a number it wouldn&rsquo;t do anything to it and would still print out 16. However, for negative numbers it would make them bigger (that is, make them approach zero), in an order of 100 times. The following image may better clarify what Leaky ReLU does, but if you still didn&rsquo;t understand it, just think of it as a special layer in our neural network that prevents gradients from dying out during training, improving the quality of our Generator.</p><p><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/leaky-relu.png?raw=true" alt="Leaky ReLU image"></p><p>The almost-last layer (<code>Output layer with tanh activation</code>) of our neural network is the one that gives us images, although in the format of 28x28 (which equals 784) by 1, and uses the tanh activation function. This activation function formats the pixel values of the image to be in -1 to 1 range. This gives our generator the ability to produce crisper images.</p><p>Let&rsquo;s have a better understanding of what I meant by this pixel range (-1 to 1). Every image is made up by numbers, even our beautiful MNIST handwritten digits! To see those numbers, let&rsquo;s print out the image instead of showing it&mldr;</p><p>Previously we displayed the number five, which is the zeroth image of the MNIST dataset, using matplotlib:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>X_train</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/x_train[0].png?raw=true" alt="MNIST Dataset"></p><p>Seeing the numbers that make up this image is easy:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X_train</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136
</span></span><span class=line><span class=cl>  175  26 166 255 247 127   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253
</span></span><span class=line><span class=cl>  225 172 253 242 195  64   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251
</span></span><span class=line><span class=cl>   93  82  82  56  39   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119
</span></span><span class=line><span class=cl>   25   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253
</span></span><span class=line><span class=cl>  150  27   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252
</span></span><span class=line><span class=cl>  253 187   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249
</span></span><span class=line><span class=cl>  253 249  64   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253
</span></span><span class=line><span class=cl>  253 207   2   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253
</span></span><span class=line><span class=cl>  250 182   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201
</span></span><span class=line><span class=cl>   78   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]]</span></span></code></pre></div><p>We can almost see the number five being formed by the combination of the numbers&mldr; These numbers are in grayscale (0 to 255 range), and when used <code>cmap='gray'</code> in matplotlib, it will show a black and white image like the one we got before. Now you understand that changing the range of the pixel values, changes the range of the numbers that make up an image (tanh would transform 0 to 255, to -1 to 1). We will see more of this in the <a href=#training>training subsection</a>.</p><p>Now the real-last layer (<code>Reshape</code>) of our neural network simply reshapes the images which are in 784 by 1 to our normal size images: 28 by 28. That is, before getting into this last layer, we had some sort of array in the dimensions 784x1. Watch out for the difference:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X_train</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>(28, 28)</span></span></code></pre></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>X_train</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=mi>28</span><span class=o>*</span><span class=mi>28</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>c</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>(784, 1)</span></span></code></pre></div><p>The reshape layer takes the (784, 1) array and makes it a (28, 28) array that you just saw above!</p><h4 id=code>Code<a hidden class=anchor aria-hidden=true href=#code>#</a></h4><p>Let us finally get into the code for creating this Generator.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.models</span> <span class=kn>import</span> <span class=n>Sequential</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.layers</span> <span class=kn>import</span> <span class=n>Dense</span><span class=p>,</span> <span class=n>Flatten</span><span class=p>,</span> <span class=n>Reshape</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.layers</span> <span class=kn>import</span> <span class=n>LeakyReLU</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_generator</span><span class=p>(</span><span class=n>img_shape</span><span class=p>,</span> <span class=n>z_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Fully connected layer</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>input_dim</span><span class=o>=</span><span class=n>z_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Leaky ReLU activation</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.01</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Output layer with tanh activation</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>28</span> <span class=o>*</span> <span class=mi>28</span> <span class=o>*</span> <span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;tanh&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Reshape the Generator output to image dimensions</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Reshape</span><span class=p>(</span><span class=n>img_shape</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span><span class=line><span class=cl>    
</span></span></code></pre></td></tr></table></div></div><h3 id=discriminator>Discriminator<a hidden class=anchor aria-hidden=true href=#discriminator>#</a></h3><h4 id=short-explanation-1>Short explanation<a hidden class=anchor aria-hidden=true href=#short-explanation-1>#</a></h4><p>You may notice the Discriminator and the Generator are very similar networks, however in most GANs implementations they often very greatly in both size and complexity.</p><p>The first layer (<code>Flatten</code>) takes in the images in the format of 28x28 and reshapes them to 784x1. Now you might ask me why we went through the small hassle of adding another layer in our GAN model for reshaping since our work would be &ldquo;undone&rdquo; when it got to the input of the discriminator. The reason we reshaped the output of the Generator network was for its images to blend in with the real images which are already in the 28x28 format, regardless of the Discriminator network taking in 784x1 images as input. However, you could in theory simply reshape the MNIST dataset images to 784x1, and not need to reshape the generated images to 28x28 (just leave it at 784x1 as well), then the real images and the fake ones would still blend in (just not in the 28x28 format).</p><p>Then, the reshaped images will go into a <code>Fully connected layer</code> of 128 units. Then it gets connected to our first and only hidden layer that uses a <code>Leaky ReLU</code> activation function. This part is identical to the Generator network.</p><p>The last layer will output probabilities, therefore we are using the <code>Sigmoid</code> activation function, which maps all the outputs in the range of 0 to 1.</p><h4 id=code-1>Code<a hidden class=anchor aria-hidden=true href=#code-1>#</a></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_discriminator</span><span class=p>(</span><span class=n>img_shape</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Flatten the input image</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Flatten</span><span class=p>(</span><span class=n>input_shape</span><span class=o>=</span><span class=n>img_shape</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Fully connected layer</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Leaky ReLU activation</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.01</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Output layer with sigmoid activation</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=building-the-model>Building the Model<a hidden class=anchor aria-hidden=true href=#building-the-model>#</a></h3><h4 id=short-explanation-2>Short explanation<a hidden class=anchor aria-hidden=true href=#short-explanation-2>#</a></h4><p>In <code>build_gan()</code> we add Generator and Discriminator together in our <code>model</code>.</p><p>The Discriminator is compiled alone, taking as input <code>img_shape</code> (which we defined earlier) and the loss being computed using <code>binary_crossentropy'</code>. Binary cross-entropy is a measure of the difference between computed probabilities and actual probabilities for predictions with only two possible classes. In our case the possible classes are real or fake.</p><p>Therefore, binary cross-entropy is going to tell us how off the prediction of the Discriminator is. Remember the Discriminator&rsquo;s goal is to not be off in the predictions, while the Generator wants the Discriminator to be very off when it comes to fake images predictions. This also means the Discriminator wants to minimize its loss for real and fake images, and the Generator wants to maximize the Discriminator loss for fake images.</p><p>The <code>optimizer</code> argument is required for compiling the model, although I don&rsquo;t know how the <code>Adam()</code> optimizer works, it has become the default in many GAN implementations due to its often superior performance. However, it suffices to say the optimizer is the one who is responsible for updating the weights and learning rates of the neural network.</p><p>The <code>'accuracy'</code> metrics is the way the Discriminator, and later us, will know how well it is doing.</p><p>The Generator is built taking as input <code>img_shape</code> and the <code>z_dim</code> arguments we defined earlier. Then, we only compile the Generator together with the Discriminator in the <code>gan</code> argument. For doing this, we first neet to set <code>discriminator.trainable</code> to <code>false</code>.</p><p>To clarify what I just stated, notice how below we <strong>built</strong> and <strong>compiled</strong> the Discriminator alone. This means that when we use the Discriminator and train it, it will take care of its own weights without interfering with the Generator network. Since it is alone, it can&rsquo;t interfere with anyone else. In turn, the generator is <strong>built</strong> alone, but it is <strong>compiled</strong> only when it is together with the Discriminator. It is only possible to train the Generator without tweaking the Discriminator weights by mistake if we set <code>discriminator.trainable</code> to false.</p><p>Lastly, we save a file called <code>generator_model.h5</code> containing the Generator network model. After trained, we can even use this file in other projects with the same abilities our generative model will have.</p><h4 id=code-2>Code<a hidden class=anchor aria-hidden=true href=#code-2>#</a></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_gan</span><span class=p>(</span><span class=n>generator</span><span class=p>,</span> <span class=n>discriminator</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Combined Generator -&gt; Discriminator model</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>generator</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>discriminator</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.optimizers</span> <span class=kn>import</span> <span class=n>Adam</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build and compile the Discriminator</span>
</span></span><span class=line><span class=cl><span class=n>discriminator</span> <span class=o>=</span> <span class=n>build_discriminator</span><span class=p>(</span><span class=n>img_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>discriminator</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=n>optimizer</span><span class=o>=</span><span class=n>Adam</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                      <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build the Generator</span>
</span></span><span class=line><span class=cl><span class=n>generator</span> <span class=o>=</span> <span class=n>build_generator</span><span class=p>(</span><span class=n>img_shape</span><span class=p>,</span> <span class=n>z_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Keep Discriminator’s parameters constant for Generator training</span>
</span></span><span class=line><span class=cl><span class=n>discriminator</span><span class=o>.</span><span class=n>trainable</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build and compile GAN model with fixed Discriminator to train the Generator</span>
</span></span><span class=line><span class=cl><span class=n>gan</span> <span class=o>=</span> <span class=n>build_gan</span><span class=p>(</span><span class=n>generator</span><span class=p>,</span> <span class=n>discriminator</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gan</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=n>Adam</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generator</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s1>&#39;generator_model.h5&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.</span></span></code></pre></div><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><h4 id=short-explanation-3>Short explanation<a hidden class=anchor aria-hidden=true href=#short-explanation-3>#</a></h4><p>Next, we will build a training loop for our GAN.</p><p>The following command only gives us the <code>X_train</code> dataset from MNIST dataset. You can see that all the 60k images in the format of 28x28 are there.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>_</span><span class=p>),</span> <span class=p>(</span><span class=n>_</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=o>=</span> <span class=n>mnist</span><span class=o>.</span><span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X_train</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>(60000, 28, 28)</span></span></code></pre></div><p>However, they are still in the 0 to 255 range we saw earlier.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X_train</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136
</span></span><span class=line><span class=cl>  175  26 166 255 247 127   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253
</span></span><span class=line><span class=cl>  225 172 253 242 195  64   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251
</span></span><span class=line><span class=cl>   93  82  82  56  39   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119
</span></span><span class=line><span class=cl>   25   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253
</span></span><span class=line><span class=cl>  150  27   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252
</span></span><span class=line><span class=cl>  253 187   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249
</span></span><span class=line><span class=cl>  253 249  64   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253
</span></span><span class=line><span class=cl>  253 207   2   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253
</span></span><span class=line><span class=cl>  250 182   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201
</span></span><span class=line><span class=cl>   78   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]
</span></span><span class=line><span class=cl> [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
</span></span><span class=line><span class=cl>    0   0   0   0   0   0   0   0   0   0]]</span></span></code></pre></div><p>The issue is that the real images are very distinguishable from the fake ones, since we used <code>tanh</code> activation function in the Generator, it is working with images that go from -1 to 1. That means that the fake images are in this -1 to 1 range, while the real ones are in 0 to 255. Very easy to spot their differences like this. Let us fix this by setting the range of values of the real images to <strong>also</strong> be in -1 to 1.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X_train</span> <span class=o>=</span> <span class=n>X_train</span> <span class=o>/</span> <span class=mf>127.5</span> <span class=o>-</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl><span class=n>X_train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Now we print another 5 from the real dataset to check if they are correct&mldr;</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X_train</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>[[[-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> [[-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> [[-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> [[-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> [[-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> [[-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-0.97647059]
</span></span><span class=line><span class=cl>  [-0.85882353]
</span></span><span class=line><span class=cl>  [-0.85882353]
</span></span><span class=line><span class=cl>  [-0.85882353]
</span></span><span class=line><span class=cl>  [-0.01176471]
</span></span><span class=line><span class=cl>  [ 0.06666667]
</span></span><span class=line><span class=cl>  [ 0.37254902]
</span></span><span class=line><span class=cl>  [-0.79607843]
</span></span><span class=line><span class=cl>  [ 0.30196078]
</span></span><span class=line><span class=cl>  [ 1.        ]
</span></span><span class=line><span class=cl>  [ 0.9372549 ]
</span></span><span class=line><span class=cl>  [-0.00392157]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> [[-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-0.76470588]
</span></span><span class=line><span class=cl>  [-0.71764706]
</span></span><span class=line><span class=cl>  [-0.2627451 ]
</span></span><span class=line><span class=cl>  [ 0.20784314]
</span></span><span class=line><span class=cl>  [ 0.33333333]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.76470588]
</span></span><span class=line><span class=cl>  [ 0.34901961]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.89803922]
</span></span><span class=line><span class=cl>  [ 0.52941176]
</span></span><span class=line><span class=cl>  [-0.49803922]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl> [[-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-0.61568627]
</span></span><span class=line><span class=cl>  [ 0.86666667]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.98431373]
</span></span><span class=line><span class=cl>  [ 0.96862745]
</span></span><span class=line><span class=cl>  [-0.27058824]
</span></span><span class=line><span class=cl>  [-0.35686275]
</span></span><span class=line><span class=cl>  [-0.35686275]
</span></span><span class=line><span class=cl>  [-0.56078431]
</span></span><span class=line><span class=cl>  [-0.69411765]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]]
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  [[...19 OTHERS...]]  
</span></span><span class=line><span class=cl> [[-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]
</span></span><span class=line><span class=cl>  [-1.        ]]]</span></span></code></pre></div><p>Woosh, that was big. I even had to truncate 19 of these so it wouldn&rsquo;t bother you too much. However, notice in the ones that remained how they were never less than -1 or greater than 1. It doesn&rsquo;t look like a five because we lost the spaces when printing it out, but what matters is the numbers and that this image is still 28x28. I removed 19 units from the rows and columns, but if you count the ones that remained (that is 9 units), you realize that they total 28 (19+9). In fact, if I try to render this -1 to 1 image again, you will see it is still a five!</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>X_train</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/x_train[0]_ranged.png?raw=true" alt="A reranged 5"></p><p>Next with <code>real = np.ones((batch_size, 1))</code> and <code>fake = np.zeros((batch_size, 1))</code> we are creating a 1-dimensional numpy array that will be our labels for real and fake images. As the code suggest, we are encoding real as 1 and fake as 0.</p><h4 id=training-for-loop>Training for loop<a hidden class=anchor aria-hidden=true href=#training-for-loop>#</a></h4><p>Our training for loop is very important. It defines the steps that will be followed during training.</p><p>First, we are training the discriminator. We get a random batch of real images. The code line <code>idx = np.random.randint(0, X_train.shape[0], batch_size)</code> will give an array of random numbers any time it is read. Mine for once was the below, which you can see by running <code>idx</code>.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>idx</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>array([25920, 26064, 33671, 32605,  6425, 59315, 30511, 55435, 26916,
</span></span><span class=line><span class=cl>       19323, 18101, 23335,  9861, 31422, 28701, 48697, 17212, 47408,
</span></span><span class=line><span class=cl>       51358,  6759, 58100, 26888, 15078, 18564, 21526, 30725, 24117,
</span></span><span class=line><span class=cl>       32581, 53086,  6893, 15341,  6371, 41043, 21601, 42828, 28187,
</span></span><span class=line><span class=cl>        1138, 52940, 18744, 54318, 46527, 42070, 34308, 32423, 45819,
</span></span><span class=line><span class=cl>       26281,  5715, 25884, 12345,  8417, 46373, 40419, 44005, 25802,
</span></span><span class=line><span class=cl>       52323, 36148, 34411, 59891, 48229,  4545, 54343, 51594, 39092,
</span></span><span class=line><span class=cl>       53981,  7812, 58982, 32166, 14466, 11552, 54139,  8675, 11207,
</span></span><span class=line><span class=cl>       52915, 13795, 38792,  7056, 33569, 36693, 16911, 57138, 15059,
</span></span><span class=line><span class=cl>       46433, 41947, 21047, 52353, 58981, 57461, 29683, 35477, 42228,
</span></span><span class=line><span class=cl>       47068, 56887, 43435, 15456, 24735, 53753, 29363, 31095, 54618,
</span></span><span class=line><span class=cl>       40214, 54801, 40577, 56503, 31241,  8541, 10678, 58001, 43211,
</span></span><span class=line><span class=cl>       13343, 22059, 28042, 15490, 28817, 50619, 42503, 22699, 11541,
</span></span><span class=line><span class=cl>       21757, 22338, 15620, 53041, 35000, 11242,  9518, 50624, 55582,
</span></span><span class=line><span class=cl>       47070,  1390])</span></span></code></pre></div><p>So that we can get a random batch of real images, we just assign each one of the numbers in the array you saw above to an index of an image present in the MNIST dataset. This is done by running <code>imgs = X_train[idx]</code></p><p>Similarly, by running <code>z = np.random.normal(0, 1, (batch_size, 100))</code> we get random values for the input of the generator. This is our random noise sourze (z) commented earlier. It generates 100 random numbers in the range from 0 to 1. All of this is necessary so we get randomness in the number generation. No two generated numbers will be identical because everytime <code>z = np.random.normal(0, 1, (batch_size, 100))</code> is running, we get different values in the array. You can verify what these values are by running the below.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>z</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>array([[-1.91992421e-01, -1.08401678e+00, -1.19431854e-03, ...,
</span></span><span class=line><span class=cl>         3.47946637e-01,  7.05697133e-02,  1.31822535e+00],
</span></span><span class=line><span class=cl>       [ 8.09580777e-01, -8.33102821e-01,  8.19856365e-01, ...,
</span></span><span class=line><span class=cl>        -2.71591923e-01, -4.70752500e-01, -6.09906282e-01],
</span></span><span class=line><span class=cl>       [ 4.79368301e-01, -6.22733779e-01,  6.72358083e-01, ...,
</span></span><span class=line><span class=cl>         3.19932660e-01, -1.12344273e+00, -7.53240611e-01],
</span></span><span class=line><span class=cl>       ...,
</span></span><span class=line><span class=cl>       [ 2.20391394e-01, -1.89958687e-01,  2.17267157e-01, ...,
</span></span><span class=line><span class=cl>         8.10365364e-01, -6.03790376e-01,  1.17673864e+00],
</span></span><span class=line><span class=cl>       [-1.07111602e+00, -1.99597750e-01, -3.94322883e-01, ...,
</span></span><span class=line><span class=cl>         7.82795777e-01,  1.43982877e+00, -1.33052956e+00],
</span></span><span class=line><span class=cl>       [ 2.26731520e-01, -3.90780048e-01, -1.55340947e-01, ...,
</span></span><span class=line><span class=cl>        -5.19932543e-02,  5.72968732e-01, -2.68468205e+00]])</span></span></code></pre></div><p>We then feed those random numbers in the array z to our generator, which will from them generate new images and save them in an array called <code>gen_imgs</code>. The code for that is <code>gen_imgs = generator.predict(z)</code> and after ran you can verify some properties</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>gen_imgs</span> <span class=o>=</span> <span class=n>generator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>4/4 [==============================] - 0s 3ms/step</span></span></code></pre></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>gen_imgs</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></td></tr></table></div></div><p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>(128, 28, 28, 1)</span></span></code></pre></div>Above we can see the generated images are in the format of 28x28, have 1 channel and there are 128 of them.</p><p>Next we get the two batches we randomly generated (that is the random sampling we did with MNSIT dataset and the ones created by the generator through inputing random numbers), and ask the discriminator to make predictions on their classifications, calculating how much it has mistaken and updating its weights and biases to correct that.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>        <span class=n>d_loss_real</span> <span class=o>=</span> <span class=n>discriminator</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>imgs</span><span class=p>,</span> <span class=n>real</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss_fake</span> <span class=o>=</span> <span class=n>discriminator</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>gen_imgs</span><span class=p>,</span> <span class=n>fake</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss</span><span class=p>,</span> <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>d_loss_real</span><span class=p>,</span> <span class=n>d_loss_fake</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>For the generator training part, not much changes. Except now we don&rsquo;t care about random sampling through the MNIST dataset, neither updating the discriminator&rsquo;s weights and biases, afterall this is the generator training!</p><p>We get another random array of numbers (z) by running <code>z = np.random.normal(0, 1, (batch_size, 100))</code> and ask the generator to give us images by running <code>gen_imgs = generator.predict(z)</code>. All of this we saw in the discriminator training explanation, so I won&rsquo;t go into details. The difference now is that we are using <code>g_loss = gan.train_on_batch(z, real)</code> to feed images into the compiled gan model, that has both the discriminator and generator. However, it is with the generator we are talking with because we want it to give us predictions on what could those images be (real or fake). Notice the word <code>real</code> at the end of that line of code. We are calling our images real so that the discriminator can be fooled into thinking these are real images.</p><p>The last step in the Training code is not necessary for training the GANs, but it will be useful for our understanding of them. We will save their progress for later plotting in a graph. We will also take &ldquo;snapshots&rdquo; of what the generator is producing so we can see how well it can handwrite&mldr; The frequency with which the snapshots are generated is set by the variable <code>sample_interval</code> which we define in the code from the <a href=#actually-training--inspecting-output>next session</a>.</p><h4 id=code-3>Code<a hidden class=anchor aria-hidden=true href=#code-3>#</a></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>accuracies</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>iteration_checkpoints</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>iterations</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>sample_interval</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Load the MNIST dataset</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>_</span><span class=p>),</span> <span class=p>(</span><span class=n>_</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=o>=</span> <span class=n>mnist</span><span class=o>.</span><span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Rescale [0, 255] grayscale pixel values to [-1, 1]</span>
</span></span><span class=line><span class=cl>    <span class=n>X_train</span> <span class=o>=</span> <span class=n>X_train</span> <span class=o>/</span> <span class=mf>127.5</span> <span class=o>-</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl>    <span class=n>X_train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Labels for real images: all ones</span>
</span></span><span class=line><span class=cl>    <span class=n>real</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Labels for fake images: all zeros</span>
</span></span><span class=line><span class=cl>    <span class=n>fake</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>iteration</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>iterations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># -------------------------</span>
</span></span><span class=line><span class=cl>        <span class=c1>#  Train the Discriminator</span>
</span></span><span class=line><span class=cl>        <span class=c1># -------------------------</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Get a random batch of real images</span>
</span></span><span class=line><span class=cl>        <span class=n>idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>imgs</span> <span class=o>=</span> <span class=n>X_train</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Generate a batch of fake images</span>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>100</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>gen_imgs</span> <span class=o>=</span> <span class=n>generator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Train Discriminator</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss_real</span> <span class=o>=</span> <span class=n>discriminator</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>imgs</span><span class=p>,</span> <span class=n>real</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss_fake</span> <span class=o>=</span> <span class=n>discriminator</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>gen_imgs</span><span class=p>,</span> <span class=n>fake</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss</span><span class=p>,</span> <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>d_loss_real</span><span class=p>,</span> <span class=n>d_loss_fake</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ---------------------</span>
</span></span><span class=line><span class=cl>        <span class=c1>#  Train the Generator</span>
</span></span><span class=line><span class=cl>        <span class=c1># ---------------------</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Generate a batch of fake images</span>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>100</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>gen_imgs</span> <span class=o>=</span> <span class=n>generator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Train Generator</span>
</span></span><span class=line><span class=cl>        <span class=n>g_loss</span> <span class=o>=</span> <span class=n>gan</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>real</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>iteration</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=n>sample_interval</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Save losses and accuracies so they can be plotted after training</span>
</span></span><span class=line><span class=cl>            <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>d_loss</span><span class=p>,</span> <span class=n>g_loss</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>accuracies</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=mf>100.0</span> <span class=o>*</span> <span class=n>accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>iteration_checkpoints</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>iteration</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Output training progress</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=si>%d</span><span class=s2> [D loss: </span><span class=si>%f</span><span class=s2>, acc.: </span><span class=si>%.2f%%</span><span class=s2>] [G loss: </span><span class=si>%f</span><span class=s2>]&#34;</span> <span class=o>%</span>
</span></span><span class=line><span class=cl>                  <span class=p>(</span><span class=n>iteration</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>d_loss</span><span class=p>,</span> <span class=mf>100.0</span> <span class=o>*</span> <span class=n>accuracy</span><span class=p>,</span> <span class=n>g_loss</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Output a sample of generated image</span>
</span></span><span class=line><span class=cl>            <span class=n>sample_images</span><span class=p>(</span><span class=n>generator</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>sample_images</span><span class=p>(</span><span class=n>generator</span><span class=p>,</span> <span class=n>image_grid_rows</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>image_grid_columns</span><span class=o>=</span><span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Sample random noise</span>
</span></span><span class=line><span class=cl>    <span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>image_grid_rows</span> <span class=o>*</span> <span class=n>image_grid_columns</span><span class=p>,</span> <span class=n>z_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Generate images from random noise</span>
</span></span><span class=line><span class=cl>    <span class=n>gen_imgs</span> <span class=o>=</span> <span class=n>generator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Rescale image pixel values to [0, 1]</span>
</span></span><span class=line><span class=cl>    <span class=n>gen_imgs</span> <span class=o>=</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>gen_imgs</span> <span class=o>+</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Set image grid</span>
</span></span><span class=line><span class=cl>    <span class=n>fig</span><span class=p>,</span> <span class=n>axs</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>image_grid_rows</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=n>image_grid_columns</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>4</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                            <span class=n>sharey</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=n>sharex</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>cnt</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>image_grid_rows</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>image_grid_columns</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># Output a grid of images</span>
</span></span><span class=line><span class=cl>            <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>gen_imgs</span><span class=p>[</span><span class=n>cnt</span><span class=p>,</span> <span class=p>:,</span> <span class=p>:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s1>&#39;off&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>cnt</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=extra-explanation>Extra explanation<a hidden class=anchor aria-hidden=true href=#extra-explanation>#</a></h4><p>I am adding this section to clarify some things and hopefully answer my mentor&rsquo;s inquiries.</p><p>He noticed a weirdness in training with the MNIST dataset. There are 10 numbers, ranging from 0 to 9, which means they all look different! And each step of training we are not defining the numbers we are using that time, so we are not getting our generator to be better at generating 1s or 6s. What we are in fact doing is modeling random noise (from our z) with the statistical properties of real data. These properties are the distribution of the numbers that make up the real images, and things like pixel values, and the spatial correlations between neighboring pixels.</p><p>That means we are not training our GAN to create any number better specifically, but for all of them in some way to end up looking more real. The specificity in our GAN approach would have made the generator network create a one number better than all the others, our eyes passing images from the generated number as real but reproving all the other number generation attempts (why does this 0 look like a 5???)</p><p>If we wanted better looking 0s, 1s, 2s, 3s&mldr; so that they don&rsquo;t overall look real, but each one of them looked super real we would have to go with another GAN approach known as conditional GAN (cGAN).</p><p>Another thing he noticed, and hopefully you notice it too is that the snapshot images (which you can see in the <a href=#actually-training--inspecting-output>next session</a>) are also random. Look at the last two 4x4 grids. If you go in the same position in the two images you will notice the GAN attempted to create something different in there. It is not the same number.</p><p><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/ex1.png?raw=true" alt="MNIST Dataset"></p><p>This can be explained because of our noise source z that adds randomness in the number generation. Even though I like this answer, if we look into the <a href=https://www.tensorflow.org/tutorials/generative/dcgan>tensorflow tutorial on Deep Convolutional Generative Adversarial Network</a>, you can see they created a GIF of those images we are plotting. How could they do that if the numbers are in different positions? Maybe the randomness from z is not a good answer afterall.</p><h3 id=actually-training--inspecting-output>Actually training + Inspecting Output<a hidden class=anchor aria-hidden=true href=#actually-training--inspecting-output>#</a></h3><h4 id=short-explanation-4>Short explanation<a hidden class=anchor aria-hidden=true href=#short-explanation-4>#</a></h4><p>We covered pretty much every part of the code. Now we are just defining the hyperparameters. While machine learning models are usually very sensible to them, our GAN model is simple and therefore is less sensible to have bad hyperparameters. Of course, your images will become pretty bad if you set them badly, but it is not the end of the world. The number of <code>iterations</code> defines how many times we will go through the training loop we just saw a code block ago. It took me one hour to run it on colab, it might take you a different time running somewhere else. An ideal iteration number for this GAN would be 100,000. However, I don&rsquo;t want to wait 5h just to get images for a practice tutorial&mldr; Maybe you don&rsquo;t want to wait that much time to learn the content either.</p><p>Batch size will tell our training loop how much images to get for the Discriminator to predict at. And lastly <code>sample_interval</code> defines after how many iterations to print us a 4x4 grid with the progress of the Generator network. That is our &ldquo;snapshot&rdquo; commented briefly in the last session.</p><p>Also, if you get a warning running this part of the code as <code>Discrepancy between trainable weights and collected trainable</code>, it is just Keras complaining we held the Discriminator&rsquo;s parameters constant while training the Generator.</p><p>The last two blocks of code are simply the ones that will use matplotlib to output us the 4x4 grid every <code>sample_interval</code></p><h4 id=code-4>Code<a hidden class=anchor aria-hidden=true href=#code-4>#</a></h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Set hyperparameters</span>
</span></span><span class=line><span class=cl><span class=n>iterations</span> <span class=o>=</span> <span class=mi>20000</span> <span class=c1># It takes 1h to run because of this high amount of interations</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>sample_interval</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Train the GAN for the specified number of iterations</span>
</span></span><span class=line><span class=cl><span class=n>train</span><span class=p>(</span><span class=n>iterations</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>sample_interval</span>
</span></span></code></pre></td></tr></table></div></div><p><div class=highlight><pre tabindex=0 class=chroma><code class=language-plaintext data-lang=plaintext><span class=line><span class=cl>Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
</span></span><span class=line><span class=cl>11490434/11490434 [==============================] - 0s 0us/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 4ms/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 4ms/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 6ms/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 4ms/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 4ms/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 4ms/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 4ms/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 5ms/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 3ms/step
</span></span><span class=line><span class=cl>4/4 [==============================] - 0s 3ms/step
</span></span><span class=line><span class=cl>... ALSO TRUNCATED HERE SO IT DOESN&#39;T TAKE MUCH SPACE...</span></span></code></pre></div><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_1.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_2.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_3.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_4.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_5.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_6.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_7.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_8.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_9.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_10.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_11.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_12.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_13.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_14.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_15.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_16.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_17.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_18.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_19.png?raw=true" alt="GAN generated digit">
<img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/gan_20.png?raw=true" alt="GAN generated digit"></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>losses</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>losses</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot training losses for Discriminator and Generator</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>losses</span><span class=o>.</span><span class=n>T</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Discriminator loss&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>losses</span><span class=o>.</span><span class=n>T</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Generator loss&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xticks</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Training Loss&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Iteration&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Loss&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/training_loss.png?raw=true" alt="Training loss graph"></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>accuracies</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>accuracies</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot Discriminator accuracy</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>accuracies</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Discriminator accuracy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xticks</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>yticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Discriminator Accuracy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Iteration&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Accuracy (%)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p><img loading=lazy src="https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/disciminator_accuracy.png?raw=true" alt="Discriminator accuracy graph"></p><h2 id=full-code>Full Code<a hidden class=anchor aria-hidden=true href=#full-code>#</a></h2><p>In case you already understand the whole code structure, feel free to just run the code provided below.</p><p>The code is also available <a href=https://github.com/luispengler/me/blob/main/static/blog/practice-understanding-gans/GAN.ipynb>here</a> as a jupyter notebook.</p><h3 id=imports>Imports<a hidden class=anchor aria-hidden=true href=#imports>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Import statements</span>
</span></span><span class=line><span class=cl><span class=o>%</span><span class=n>matplotlib</span> <span class=n>inline</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.datasets</span> <span class=kn>import</span> <span class=n>mnist</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.layers</span> <span class=kn>import</span> <span class=n>Dense</span><span class=p>,</span> <span class=n>Flatten</span><span class=p>,</span> <span class=n>Reshape</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.layers</span> <span class=kn>import</span> <span class=n>LeakyReLU</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.models</span> <span class=kn>import</span> <span class=n>Sequential</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>keras.optimizers</span> <span class=kn>import</span> <span class=n>Adam</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>### Model input dimensions</span>
</span></span><span class=line><span class=cl><span class=n>img_rows</span> <span class=o>=</span> <span class=mi>28</span>
</span></span><span class=line><span class=cl><span class=n>img_cols</span> <span class=o>=</span> <span class=mi>28</span>
</span></span><span class=line><span class=cl><span class=n>channels</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Input image dimensions</span>
</span></span><span class=line><span class=cl><span class=n>img_shape</span> <span class=o>=</span> <span class=p>(</span><span class=n>img_rows</span><span class=p>,</span> <span class=n>img_cols</span><span class=p>,</span> <span class=n>channels</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Size of the noise vector, used as input to the Generator</span>
</span></span><span class=line><span class=cl><span class=n>z_dim</span> <span class=o>=</span> <span class=mi>100</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=generator-1>Generator<a hidden class=anchor aria-hidden=true href=#generator-1>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_generator</span><span class=p>(</span><span class=n>img_shape</span><span class=p>,</span> <span class=n>z_dim</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Fully connected layer</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>,</span> <span class=n>input_dim</span><span class=o>=</span><span class=n>z_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Leaky ReLU activation</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.01</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Output layer with tanh activation</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>28</span> <span class=o>*</span> <span class=mi>28</span> <span class=o>*</span> <span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;tanh&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Reshape the Generator output to image dimensions</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Reshape</span><span class=p>(</span><span class=n>img_shape</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=discriminator-1>Discriminator<a hidden class=anchor aria-hidden=true href=#discriminator-1>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_discriminator</span><span class=p>(</span><span class=n>img_shape</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Flatten the input image</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Flatten</span><span class=p>(</span><span class=n>input_shape</span><span class=o>=</span><span class=n>img_shape</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Fully connected layer</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>128</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Leaky ReLU activation</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>LeakyReLU</span><span class=p>(</span><span class=n>alpha</span><span class=o>=</span><span class=mf>0.01</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Output layer with sigmoid activation</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>activation</span><span class=o>=</span><span class=s1>&#39;sigmoid&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=building-the-model-1>Building the Model<a hidden class=anchor aria-hidden=true href=#building-the-model-1>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_gan</span><span class=p>(</span><span class=n>generator</span><span class=p>,</span> <span class=n>discriminator</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Combined Generator -&gt; Discriminator model</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>generator</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>discriminator</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>model</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Build and compile the Discriminator</span>
</span></span><span class=line><span class=cl><span class=n>discriminator</span> <span class=o>=</span> <span class=n>build_discriminator</span><span class=p>(</span><span class=n>img_shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>discriminator</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=n>optimizer</span><span class=o>=</span><span class=n>Adam</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                      <span class=n>metrics</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;accuracy&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build the Generator</span>
</span></span><span class=line><span class=cl><span class=n>generator</span> <span class=o>=</span> <span class=n>build_generator</span><span class=p>(</span><span class=n>img_shape</span><span class=p>,</span> <span class=n>z_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Keep Discriminator’s parameters constant for Generator training</span>
</span></span><span class=line><span class=cl><span class=n>discriminator</span><span class=o>.</span><span class=n>trainable</span> <span class=o>=</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Build and compile GAN model with fixed Discriminator to train the Generator</span>
</span></span><span class=line><span class=cl><span class=n>gan</span> <span class=o>=</span> <span class=n>build_gan</span><span class=p>(</span><span class=n>generator</span><span class=p>,</span> <span class=n>discriminator</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gan</span><span class=o>.</span><span class=n>compile</span><span class=p>(</span><span class=n>loss</span><span class=o>=</span><span class=s1>&#39;binary_crossentropy&#39;</span><span class=p>,</span> <span class=n>optimizer</span><span class=o>=</span><span class=n>Adam</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generator</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s1>&#39;generator_model.h5&#39;</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=training-1>Training<a hidden class=anchor aria-hidden=true href=#training-1>#</a></h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span><span class=lnt>58
</span><span class=lnt>59
</span><span class=lnt>60
</span><span class=lnt>61
</span><span class=lnt>62
</span><span class=lnt>63
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>accuracies</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>iteration_checkpoints</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train</span><span class=p>(</span><span class=n>iterations</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>sample_interval</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Load the MNIST dataset</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>_</span><span class=p>),</span> <span class=p>(</span><span class=n>_</span><span class=p>,</span> <span class=n>_</span><span class=p>)</span> <span class=o>=</span> <span class=n>mnist</span><span class=o>.</span><span class=n>load_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Rescale [0, 255] grayscale pixel values to [-1, 1]</span>
</span></span><span class=line><span class=cl>    <span class=n>X_train</span> <span class=o>=</span> <span class=n>X_train</span> <span class=o>/</span> <span class=mf>127.5</span> <span class=o>-</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl>    <span class=n>X_train</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>X_train</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Labels for real images: all ones</span>
</span></span><span class=line><span class=cl>    <span class=n>real</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Labels for fake images: all zeros</span>
</span></span><span class=line><span class=cl>    <span class=n>fake</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>iteration</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>iterations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># -------------------------</span>
</span></span><span class=line><span class=cl>        <span class=c1>#  Train the Discriminator</span>
</span></span><span class=line><span class=cl>        <span class=c1># -------------------------</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Get a random batch of real images</span>
</span></span><span class=line><span class=cl>        <span class=n>idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>X_train</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>imgs</span> <span class=o>=</span> <span class=n>X_train</span><span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Generate a batch of fake images</span>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>100</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>gen_imgs</span> <span class=o>=</span> <span class=n>generator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Train Discriminator</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss_real</span> <span class=o>=</span> <span class=n>discriminator</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>imgs</span><span class=p>,</span> <span class=n>real</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss_fake</span> <span class=o>=</span> <span class=n>discriminator</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>gen_imgs</span><span class=p>,</span> <span class=n>fake</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>d_loss</span><span class=p>,</span> <span class=n>accuracy</span> <span class=o>=</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>add</span><span class=p>(</span><span class=n>d_loss_real</span><span class=p>,</span> <span class=n>d_loss_fake</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ---------------------</span>
</span></span><span class=line><span class=cl>        <span class=c1>#  Train the Generator</span>
</span></span><span class=line><span class=cl>        <span class=c1># ---------------------</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Generate a batch of fake images</span>
</span></span><span class=line><span class=cl>        <span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>batch_size</span><span class=p>,</span> <span class=mi>100</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>gen_imgs</span> <span class=o>=</span> <span class=n>generator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Train Generator</span>
</span></span><span class=line><span class=cl>        <span class=n>g_loss</span> <span class=o>=</span> <span class=n>gan</span><span class=o>.</span><span class=n>train_on_batch</span><span class=p>(</span><span class=n>z</span><span class=p>,</span> <span class=n>real</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>iteration</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span> <span class=o>%</span> <span class=n>sample_interval</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Save losses and accuracies so they can be plotted after training</span>
</span></span><span class=line><span class=cl>            <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>d_loss</span><span class=p>,</span> <span class=n>g_loss</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>accuracies</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=mf>100.0</span> <span class=o>*</span> <span class=n>accuracy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>iteration_checkpoints</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>iteration</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Output training progress</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=si>%d</span><span class=s2> [D loss: </span><span class=si>%f</span><span class=s2>, acc.: </span><span class=si>%.2f%%</span><span class=s2>] [G loss: </span><span class=si>%f</span><span class=s2>]&#34;</span> <span class=o>%</span>
</span></span><span class=line><span class=cl>                  <span class=p>(</span><span class=n>iteration</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=n>d_loss</span><span class=p>,</span> <span class=mf>100.0</span> <span class=o>*</span> <span class=n>accuracy</span><span class=p>,</span> <span class=n>g_loss</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># Output a sample of generated image</span>
</span></span><span class=line><span class=cl>            <span class=n>sample_images</span><span class=p>(</span><span class=n>generator</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>sample_images</span><span class=p>(</span><span class=n>generator</span><span class=p>,</span> <span class=n>image_grid_rows</span><span class=o>=</span><span class=mi>4</span><span class=p>,</span> <span class=n>image_grid_columns</span><span class=o>=</span><span class=mi>4</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Sample random noise</span>
</span></span><span class=line><span class=cl>    <span class=n>z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=p>(</span><span class=n>image_grid_rows</span> <span class=o>*</span> <span class=n>image_grid_columns</span><span class=p>,</span> <span class=n>z_dim</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Generate images from random noise</span>
</span></span><span class=line><span class=cl>    <span class=n>gen_imgs</span> <span class=o>=</span> <span class=n>generator</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Rescale image pixel values to [0, 1]</span>
</span></span><span class=line><span class=cl>    <span class=n>gen_imgs</span> <span class=o>=</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>gen_imgs</span> <span class=o>+</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Set image grid</span>
</span></span><span class=line><span class=cl>    <span class=n>fig</span><span class=p>,</span> <span class=n>axs</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>image_grid_rows</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=n>image_grid_columns</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=mi>4</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                            <span class=n>sharey</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                            <span class=n>sharex</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>cnt</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>image_grid_rows</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>image_grid_columns</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=c1># Output a grid of images</span>
</span></span><span class=line><span class=cl>            <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>gen_imgs</span><span class=p>[</span><span class=n>cnt</span><span class=p>,</span> <span class=p>:,</span> <span class=p>:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>axs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s1>&#39;off&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>cnt</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=actually-training--inspecting-output-1>Actually training + Inspecting Output<a hidden class=anchor aria-hidden=true href=#actually-training--inspecting-output-1>#</a></h3><p>Note that the <code>'Discrepancy between trainable weights and collected trainable'</code> warning from Keras is expected. It is by design: The Generator&rsquo;s trainable parameters are intentionally held constant during Discriminator training, and vice versa.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Set hyperparameters</span>
</span></span><span class=line><span class=cl><span class=n>iterations</span> <span class=o>=</span> <span class=mi>20000</span> <span class=c1># It takes 1h to run because of this high amount of interations</span>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>128</span>
</span></span><span class=line><span class=cl><span class=n>sample_interval</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Train the GAN for the specified number of iterations</span>
</span></span><span class=line><span class=cl><span class=n>train</span><span class=p>(</span><span class=n>iterations</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>,</span> <span class=n>sample_interval</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>losses</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>losses</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot training losses for Discriminator and Generator</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>losses</span><span class=o>.</span><span class=n>T</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Discriminator loss&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>losses</span><span class=o>.</span><span class=n>T</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Generator loss&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xticks</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Training Loss&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Iteration&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Loss&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>accuracies</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>accuracies</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot Discriminator accuracy</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>accuracies</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Discriminator accuracy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xticks</span><span class=p>(</span><span class=n>iteration_checkpoints</span><span class=p>,</span> <span class=n>rotation</span><span class=o>=</span><span class=mi>90</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>yticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s2>&#34;Discriminator Accuracy&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s2>&#34;Iteration&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s2>&#34;Accuracy (%)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div></div><footer class=post-footer><ul class=post-tags></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Practice: Understanding GANs With MNIST Dataset on twitter" href="https://twitter.com/intent/tweet/?text=Practice%3a%20Understanding%20GANs%20With%20MNIST%20Dataset&amp;url=https%3a%2f%2fluispengler.github.io%2fme%2fblog%2fpractice-understanding-gans%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Practice: Understanding GANs With MNIST Dataset on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fluispengler.github.io%2fme%2fblog%2fpractice-understanding-gans%2f&amp;title=Practice%3a%20Understanding%20GANs%20With%20MNIST%20Dataset&amp;summary=Practice%3a%20Understanding%20GANs%20With%20MNIST%20Dataset&amp;source=https%3a%2f%2fluispengler.github.io%2fme%2fblog%2fpractice-understanding-gans%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Practice: Understanding GANs With MNIST Dataset on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fluispengler.github.io%2fme%2fblog%2fpractice-understanding-gans%2f&title=Practice%3a%20Understanding%20GANs%20With%20MNIST%20Dataset"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Practice: Understanding GANs With MNIST Dataset on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fluispengler.github.io%2fme%2fblog%2fpractice-understanding-gans%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Practice: Understanding GANs With MNIST Dataset on whatsapp" href="https://api.whatsapp.com/send?text=Practice%3a%20Understanding%20GANs%20With%20MNIST%20Dataset%20-%20https%3a%2f%2fluispengler.github.io%2fme%2fblog%2fpractice-understanding-gans%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Practice: Understanding GANs With MNIST Dataset on telegram" href="https://telegram.me/share/url?text=Practice%3a%20Understanding%20GANs%20With%20MNIST%20Dataset&amp;url=https%3a%2f%2fluispengler.github.io%2fme%2fblog%2fpractice-understanding-gans%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://luispengler.github.io/me>Luis Spengler - Webpage</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>